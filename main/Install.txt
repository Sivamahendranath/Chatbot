ðŸ§  What is Ollama?
Ollama lets you run large language models (LLMs) like Mistral, LLaMA2, and Gemma locally on your machine, with no internet required after setup.

âœ… Step-by-Step Installation of Ollama
ðŸ”¹ Step 1: Visit Ollama Website
Go to https://ollama.com

ðŸ”¹ Step 2: Download Ollama Installer
Choose the appropriate installer based on your OS:

Windows â€“ .exe installer

macOS â€“ .pkg installer

Linux â€“ Install via terminal (see below)

ðŸ”¹ Step 3: Install Ollama
âœ… On Windows/macOS
Run the downloaded installer

Follow the on-screen instructions to install Ollama

Once installed, it runs a local server at http://localhost:11434

âœ… On Linux (Debian/Ubuntu)
Open a terminal and run:

bash
Copy
Edit
curl -fsSL https://ollama.com/install.sh | sh
Then start Ollama:

bash
Copy
Edit
ollama serve
ðŸ”¹ Step 4: Pull a Model
After installation, open a terminal and pull a model. Example:

bash
Copy
Edit
ollama pull mistral
This will download the Mistral model (a fast, lightweight LLM). You can also use:

ollama pull llama2

ollama pull gemma

ollama pull codellama

ðŸ”¹ Step 5: Confirm Itâ€™s Running
You can check if Ollama is active by visiting:

bash
Copy
Edit
http://localhost:11434/api/tags
Or run:

bash
Copy
Edit
ollama list
It should show installed models.

ðŸ”¹ Step 6: Start the Ollama Server (if not already running)
bash
Copy
Edit
ollama serve
You can keep this running in the background while using your chatbot app.

ðŸ’¬ Example: Running With Your Chatbot
In your Python code or Streamlit app, the chatbot connects to the local Ollama server at:

arduino
Copy
Edit
http://localhost:11434
When your chatbot calls the LLM, Ollama responds with answers from the local model â€” no internet required.

ðŸ§ª Optional: Test a Model
You can chat with a model directly from the terminal:

bash
Copy
Edit
ollama run mistral
